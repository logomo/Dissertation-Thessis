\section{\secState{D}Basic Definitions}\label{s:basicDefinitions}
\noindent A few definitions are needed.

\subsection{\secState{D}World}\label{s:World} 
    \noindent The world of interest consists of:

    \emph{Space} has a Global Reference Frame with three axes $X^+,Y^+,Z^+$. 
    \begin{equation}\label{eq:SpaceDefinition}
        Space \subseteq \R^3:\text{main axes:} X^+,Y^+,Z^+\text{, center} [x_0,y_0,z_0] 
    \end{equation}

    \emph{Object} (\ref{eq:ObjectDefinition}) is generic subset of \emph{Space} which has \emph{Boundary}.
    \begin{equation}\label{eq:ObjectDefinition}
        Object \subset Space: \forall point \in Object, point \text{ is solid}
    \end{equation}

    The dynamic model of the \emph{UAV} is given by a nonlinear first order state-space model:
    \begin{equation}\label{eq:vehicleModelAbstract}
        \dot{x} = f(t,x,u)
    \end{equation}

    \noindent Where $x\in\R^{6+n}, n\in\N^{0+}$ is \emph{system state} containing minimal information about vehicle position $[x,y,z]$ and orientation $[\theta,\varphi,\rho]$ (roll, pitch yaw), and $u(t)$ is \emph{control signal} belonging to $R^k,k\in\N^+ $, bounded by control set $u(t)\in U$.
    
    The map of the terrain is given by \emph{TerrainMap} mapping $(x,y)$ to the terrain elevation
    \begin{equation}\label{eq:TerrainMap}
        TerrainMap: (x,y) \rightarrow z 
    \end{equation}
   
    \noindent\emph{Weather} (\ref{eq:weatherProjection}) can impact the performance of the \emph{UAV}. \emph{wind} can decrease flying capabilities and maneuverability, \emph{visibility} can impair the performance of sensors such as LiDAR and cameras, \emph{humidity} can be serious danger to electronic equipment and in combination low \emph{temperature} it can cause icing on vehicle, and \emph{air pressure} can impact ranging sensor and flight performance. 
    \begin{equation}\label{eq:weatherProjection}
        Weather:\left(position\in Space\right) \times time \to
        \left[
        \begin{aligned}
            \text{wind } & \begin{aligned}&(w_x,w_y,w_z)\\&[m/s,m/s,m/s]\end{aligned}\\
            \text{visibility }& v\,[m]\\
            \text{humidity }& h\,[0-100 \%] \\
            \text{air pressure }& a\,[hPa]\\
            \text{temperature: }& \tau\,[C]\\
        \end{aligned}
        \right]
    \end{equation}

\subsection{\secState{D}Mission}\label{s:mission}
    \emph{Mission} (\ref{eq:missionAbstractSet}) which UAV should fly is given as set of \emph{ordered, feasible in terms of vehicle dynamic (\ref{eq:vehicleModelAbstract}), waypoints} in subspace of $Space$ (\ref{eq:SpaceDefinition}).
    
    \begin{equation}\label{eq:missionAbstractSet}
        Mission = \left\{
        \begin{aligned}
            &waypoint_1, waypoint_2, \dots,waypoint_m:\\
            &\forall_{i=1\dots m} waypoint_i \in  Space
        \end{aligned}
        \right\}, \quad m\in\N^{+},m\ge2
    \end{equation}

    \emph{Waypoint Passing} (\ref{eq:waypointPassingFunction}) function maps system (\ref{eq:vehicleModelAbstract}) trajectory \emph{projection in Space} and \emph{Mission} waypoints (\ref{eq:missionAbstractSet}) to a vector of \emph{passing times}.
    
    \begin{equation}\label{eq:waypointPassingFunction}
        WaypointPassing:TrajectoryProjection \times Mission \to Time^m
    \end{equation}

    \begin{note}
        The \emph{Mission} (\ref{eq:missionAbstractSet}) is considered as successfully completed if and only if $\forall$ waypoints are reached and in given order (check output of \ref{eq:waypointPassingFunction}).
    \end{note}



\subsection{\secState{D}Flight Constraints}\label{s:FlightConstraints}

    \noindent Flight constraints arise from aviation rules and from ATM. 

    There are $H$ sets of hard flight constraints expressed as subsets of \emph{Space}:

    \begin{equation}\label{eq:HFlightConstraints}
        HFlightConstraints=\left\{HFlightConstraint_1, \dots, HFlightConstraint_H \right\}
    \end{equation}

    The union of all \emph{$HFlightConstraint_i$} is the set \emph{$HFlightConstraint$}. \emph{Example:} turning radius of UAV, climb rate etc. 
    
    There are $S$ sets of soft flight constraints expressed as subsets of \emph{Space}:

    \begin{equation}\label{eq:SFlightConstraints}
        SFlightConstraints=\left\{SFlightConstraint_1, \dots, SFlightConstraint_S \right\}
    \end{equation}

    The union of all \emph{$SFlightConstraint_i$} is the set \emph{SFlightConstraint}. \emph{Example:} sharp maneuvers, restricted flight areas etc.

    The \emph{Weather} may impose either soft or hard flight constraints.



\subsection{\secState{D}Partition of Space}\label{s:partitionOfSpace}

    \noindent In what follows it is convenient at each time $t$ to partition the \emph{Space} into four disjoint subsets \emph{Free(t)}, \emph{Restricted(t)}, \emph{Occupied(t)} and \emph{Uncertain(t)}.

    There is a \emph{SpaceClassification}  function (\ref{eq:spaceCassificationFunction})
    
    \begin{equation}\label{eq:spaceCassificationFunction}
        SpaceClassification: y \in \emph{Space} \mapsto s \in \{Free, Restricted, Occupied, Uncertain \}
    \end{equation}

    \begin{note}
        \emph{SpaceClassification} (\ref{eq:spaceCassificationFunction}) is \emph{total} function mapping each element of \emph{Space}. Other followup \emph{classifications} are considered as \emph{total} functions. 
    \end{note}

    There is set of \emph{Space} Hard constraints, where one \emph{HardConstraint} (\ref{eq:hardSpaceConstraints}) is given as:

    \begin{equation}\label{eq:hardSpaceConstraints}
        \emph{HardConstraint} = \{point \in \emph{Space}: \emph{SpaceClassification}(point) = Occupied \}
    \end{equation}

    There is set of \emph{Space} Soft constraints, where one \emph{SoftConstraint} (\ref{eq:softSpaceConstraints}) is given as:

    \begin{equation}\label{eq:softSpaceConstraints}
        \emph{SoftConstraint} = \{point \in \emph{Space}: \emph{SpaceClassification}(point) = Restricted \}
    \end{equation}

\subsection{\secState{D}Information Source}\label{s:informationSources}
    \begin{definition}{Information source} \label{def:InformationSurceDefinition}         
        (\ref{eq:InformationSurceDefinition}) represents a priori information for each \emph{point} in \emph{Space} at time prior mission execution mapping it into one of distinctive categories \emph{Free, Occupied, Restriced}, and \emph{Uncertain}.
        
        \begin{equation}\label{eq:InformationSurceDefinition}
            InformationSource = SpaceClassification(PriorTime)
        \end{equation}
    \end{definition}

    \begin{definition}{Landmark}\label{def:Landmark} 
        is partition of \emph{Space} (\ref{eq:SpaceDefinition}) which is notable in context of \emph{society} or \emph{law} given properties. Landmark can be: notable buildings, notable natural structures or crucial infrastructure. Landmark is part of terrain map, but it special status can induce additional properties.
    \end{definition}

    \noindent For obstacle avoidance problem following \emph{Information sources} are available:
    
    \begin{enumerate}
        \item\emph{Terrain map} - map of terrain with notable landmarks.
        \item\emph{Object map} - map of notable structures with \emph{protection zones} considered as \emph{occupied} or \emph{restricted} space.
        \item\emph{Fly zones restriction map} - map of restricted flight areas considered as \emph{restricted} constraints.
    \end{enumerate}

\subsection{\secState{D}Single Observation by Single Sensor Classification} \label{singleObservationSingle sensor}
    \noindent\emph{Vehicle} is equiped with single \emph{Sensor}, which returns \emph{Space} classification for given \emph{observation position} and observation time.


    Observation (\ref{eq:observationClassification}) at given UAV \emph{position}, \emph{time}, and for given \emph{sensor} sorts points from \emph{sensor reading} into following distinguish sets (\ref{eq:observationClassification}):
    
    \begin{enumerate}
        \item $Free_O(position,sensor,time)$ - observable space by sensor and considered as $Free$ by sensor reading,
        \item $Occupied_O(position,sensor,time)$ -  observable space by sensor and considered as $Occupied$ by sensor reading,
        \item $Uncertain_O(position,sensor,time)$ - all other points.
    \end{enumerate}
    
    \begin{equation}\label{eq:observationClassification}
        Observation(position,sensor,time)\to
        \begin{cases}
            Free_O(position,sensor,time)\\
            Occupied_O(position,sensor,time)\\
            Uncertain_O(position,sensor,time)\\
        \end{cases}
    \end{equation}


\subsection{\secState{D}Multiple Observations by Single Sensor Classification}\label{multipleObservationsBySingleSensor}
    \noindent Let Let add pairs of \emph{observation position} and \emph{observation time} in manner $\{(position_1,t_1),$ $(position_2,t_2),$ $\dots(position_k,t_k)\}, k\in\N^+$ that point position is independent and time is ordered in fashion $t_1 < t_2 < \dots < t_k$.
 
 
    \emph{Free space} from multiple sensor \emph{reading} over multiple \emph{positions} is inclusive space, because we are obtaining additional information regarding to space reachability by sensor reading independent on sensor space and orientation limitation. Therefore the union of single instances of observations are used to represent \emph{Combined free space} $Free_O(sensor)$ (\ref{eq:freeObservableSpaceForOneSensor}).

    \begin{equation}\label{eq:freeObservableSpaceForOneSensor}
        Free_O(sensor)= \bigcup_{i=\{1,\dots,k\}}Free_O(sensor,position_i,time_i)
    \end{equation}
 
    \emph{Occupied space} (\ref{eq:occupiedObservableSpaceForOneSensor}) from multiple sensor \emph{reading} over multiple \emph{positions} is inclusive space, because of increased information, therefore it has similar handling to \emph{Free space}.
    \begin{equation}\label{eq:occupiedObservableSpaceForOneSensor}
        Occupied_O(sensor)= \bigcup_{i=\{1,\dots,k\}}Occupied_O(sensor,position_i,time_i)
    \end{equation}
 
    \emph{Uncertain space} (\ref{eq:uncertainObservableSpaceForOneSensor}) from multiple sensor \emph{reading} over multiple \emph{positions} is exclusive space, because of decrease in uncertainty. 
 
    \begin{equation}\label{eq:uncertainObservableSpaceForOneSensor}
         Uncertain_O(sensor)= \bigcap_{i=\{1,\dots,k\}}Uncertain_O(sensor,position_i,time_i)
    \end{equation}

\subsection{\secState{D}Sensor Fusion}\label{s:SensorFusionDefinition}
    \noindent The observation at fixed time $t_{fix}$ can be made by multiple sensors $sensor_1,$ $sensor_2,$ $\dots,$ $sensor_k,$ $k\in\N^k$, for $k$ sensors execute observations $Observation_1$ $(sensor_1,$ $position_1,$ $t_{fix}),$ $Observation_2$ $(sensor_2,$ $position_2,$ $t_{fix}),$ $\dots,$ $Observation_k($ $sensor_k,$ $position_k,$ $t_{fix})$. 

    \emph{Sensor Fusion} (\ref{eq:SensorFusionFunction}) function with \emph{data fusion parameters} is introduced which combines \emph{Observations} for each \emph{sensor} and \emph{Weather}, then  uniquely maps each point into four distinguish sets: \emph{$Free(t_{fix})$, $Occupied(t_{fix})$, $Restricted(t_{fix})$, and $Uncertain(t_{fix})$} (special case of (\ref{eq:spaceCassificationFunction})).

    \begin{equation}\label{eq:SensorFusionFunction}
        SensorFusion:
        \left[
        \begin{aligned}
            &Observation_1 (sensor_1, position_1, t_{fix})\times\\
            &Observation_2 (sensor_2, position_2, t_{fix})\times\\
            &\times\dots\times\\
            &Observation_k (sensor_k, position_k, t_{fix})\times\\
            &Weather(\dots)\\
            &\textit{fixed time }t_{fix}\times\\
            &sensorFusionParameters\\
        \end{aligned}
        \right]
        \to
        \begin{cases}
            Free(t_{fix})\\
            Occupied(t_{fix})\\
            Restricted(t_{fix})\\
            Uncertain(t_{fix})
        \end{cases}
    \end{equation}

\subsection{\secState{D}Data Fusion}\label{s:dataFusionDefinition}
    \noindent Multiple \emph{Information Sources}: \emph{InformationSource}$_1$, \emph{InformationSource}$_2$, $\dots$, \emph{InformationSource}$_l$, $l\in\N$, where $l$ is count of information sources needs to be fused with \emph{Sensor Fusion} function (\ref{eq:SensorFusionFunction}) outcome at \emph{fixed time} $t_{fix}$.

    \emph{Data fusion} function (\ref{eq:DataFusionFunction}) combines the classification from various \emph{Information Sources} with classification from \emph{Sensor Fusion} function (\ref{eq:SensorFusionFunction}) for vehicle position at \emph{fixed time} $t_{fix}$, under given \emph{Data Fusion Parameters}. 

    \begin{equation}\label{eq:DataFusionFunction}
        DataFusion:
        \left[
        \begin{aligned}
            &InformationSource_1 \times\\
            &InformationSource_2 \times\\
            &\times\dots\times\\
            &InformationSource_l \times\\
            &SensorFusion(\dots)\times\\
            &Weather(\dots)\\
            &position\times\\
            &\textit{fixed time }t_{fix}\times\\
            &dataFusionParameters
        \end{aligned}
        \right]
        \to 
        \begin{cases}
            Free(t_{fix})\\
            Occupied(t_{fix})\\
            Restricted(t_{fix})\\
            Uncertain(t_{fix})
        \end{cases}
    \end{equation}

    Each $point$ in $Space$ is uniquely classified into one of sets\emph{Free$(t_{fix})$, Occupied$(t_{fix})$, Restricted$(t_{fix})$, Uncertain($t_{fix}$)} (special case (\ref{eq:spaceCassificationFunction})).
    
    
    \begin{note}
    Moreover \emph{Data Fusion} function is covering the case of \emph{Multiple information sources, combined with multiple sensor readings over multiple times}, including \emph{Weather} as \emph{sensor impact factor} and \emph{information source}. 
    \end{note}

\subsection{\secState{D}Known World}\label{s:KnownWorld}
    \emph{Known world} (\ref{eq:knownWorldSet}) for some \emph{fixed time}$t_{fix}$ is given as joint set of points which belongs to one of \emph{Data Fusion} (\ref{eq:DataFusionFunction}) output sets $Free(t_{fix})$ or $Occupied(t_{fix})$ or $Restricted(t_{fix})$. The \emph{Known World} is compact set with existing boundary.
    
    \begin{equation}\label{eq:knownWorldSet}
        KnownWorld(t_{fix})= Free(t_{fix}) \cup Occupied(t_{fix}) \cup Restricted(t_{fix})
    \end{equation}



\subsection{\secState{D}Safety Margin}\label{s:SafetyMarginDefinition}
    \noindent Let say that \emph{mission} was executed in \emph{time} interval $t\in [missionStart,missionEnd]$ in \emph{Known world} (\ref{eq:knownWorldSet}). For every $position$, extracted from \emph{UAV model state} $x(t)$, keeps distance from any point in $Occupied(t)$  with greater or equal to $safetyMargin$ $(s_m)$ (\ref{eq:safetyMarginAbstract}).  
    
    \begin{multline}\label{eq:safetyMarginAbstract}
        \forall t\in [missionStart,missionEnd]:\\distance(x(t),Occupied(t),t) \ge safetyMargin
    \end{multline}

